# AnÃ¡lisis Corregido: AdaptaciÃ³n de RAC-Gemini - Solo Procesamiento de Documentos

## ğŸ“‹ Resumen Ejecutivo

**Flujo Actual de BÃºsqueda (NO TOCAR):**

```
Usuario pregunta
  â†’ /api/chat
  â†’ Webhook a n8n
  â†’ n8n genera embeddings de query
  â†’ BÃºsqueda en PostgreSQL (pgvector)
  â†’ Recupera chunks relevantes
  â†’ Genera respuesta
```

**Objetivo de AdaptaciÃ³n:**

- âœ… Usar Gemini **SOLO** para el procesamiento inicial de documentos (cuando se suben)
- âœ… Mantener archivos en Supabase Storage (como actualmente)
- âœ… Guardar chunks y embeddings en PostgreSQL (esquema actual)
- âœ… **NO modificar** el flujo de bÃºsqueda/chat (n8n se mantiene)

---

## ğŸ” Flujo Actual vs Flujo Propuesto

### Flujo Actual de Procesamiento de Documentos

```
1. Usuario sube archivo
   â†“
2. Upload a Supabase Storage
   â†“
3. /api/documents/vectorize
   â†“
4. document-processor.ts:
   - Extrae texto (pdf-parse)
   - Chunking (LangChain)
   - Embeddings (OpenAI text-embedding-3-small)
   â†“
5. vector-store.ts:
   - Inserta chunks en PostgreSQL
   - Tabla: documents (content, metadata, embedding)
   â†“
6. âœ… Listo para bÃºsqueda
```

### Flujo Propuesto con Gemini

```
1. Usuario sube archivo
   â†“
2. Upload a Supabase Storage (MANTENER)
   â†“
3. /api/documents/vectorize (MANTENER)
   â†“
4. document-processor.ts (MODIFICAR):
   - Extrae texto (Gemini o mantener pdf-parse)
   - Chunking (LangChain - MANTENER)
   - Embeddings (Gemini text-embedding-004) â† CAMBIO
   â†“
5. vector-store.ts (MANTENER):
   - Inserta chunks en PostgreSQL
   - Tabla: documents (content, metadata, embedding)
   â†“
6. âœ… Listo para bÃºsqueda (n8n sigue funcionando igual)
```

### Flujo de BÃºsqueda (NO SE TOCA)

```
Usuario pregunta
   â†“
/api/chat â†’ Webhook a n8n
   â†“
n8n:
   - Genera embedding de query (OpenAI o Gemini)
   - Busca en PostgreSQL (pgvector)
   - Recupera chunks relevantes
   - Genera respuesta
   â†“
Retorna respuesta al usuario
```

---

## ğŸ¯ Cambios Necesarios

### 1. Modificar `document-processor.ts`

**Cambio principal:** Reemplazar OpenAI embeddings con Gemini embeddings

**ANTES (OpenAI):**

```typescript
import { openai } from "@ai-sdk/openai";
import { embedMany } from "ai";

const { embeddings } = await embedMany({
  model: openai.embedding("text-embedding-3-small"),
  values: chunkedContent.map((chunk) => chunk.pageContent),
});
```

**DESPUÃ‰S (Gemini):**

```typescript
// Usar Gemini REST API para embeddings
// NOTA: @google/genai SDK no tiene embeddings directos
// Necesitamos usar REST API

const embeddings = await generateEmbeddingsWithGemini(
  chunkedContent.map((chunk) => chunk.pageContent),
);
```

**Consideraciones:**

- Gemini `text-embedding-004` tiene **768 dimensiones** (no 1536)
- Necesitamos decidir:
  - OpciÃ³n A: Cambiar esquema a `vector(768)`
  - OpciÃ³n B: Mantener 1536 y usar OpenAI solo para embeddings
  - OpciÃ³n C: Usar Gemini para extracciÃ³n/chunking, OpenAI para embeddings

---

### 2. Implementar FunciÃ³n de Embeddings con Gemini

**Nueva funciÃ³n:** `src/features/documents/utils/gemini-embeddings.ts`

```typescript
import { GoogleGenAI } from "@google/genai";

/**
 * Genera embeddings usando Gemini REST API
 * Modelo: text-embedding-004 (768 dimensiones)
 */
export async function generateEmbeddingsWithGemini(
  texts: string[],
): Promise<number[][]> {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    throw new Error("GEMINI_API_KEY no estÃ¡ configurada");
  }

  // Gemini REST API para batch embeddings
  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/text-embedding-004:batchEmbedContents?key=${apiKey}`,
    {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        requests: texts.map((text) => ({
          model: "models/text-embedding-004",
          content: {
            parts: [{ text }],
          },
        })),
      }),
    },
  );

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Gemini API error: ${response.status} - ${error}`);
  }

  const data = await response.json();

  if (!data.embeddings || data.embeddings.length !== texts.length) {
    throw new Error(
      `Error: nÃºmero de embeddings (${data.embeddings?.length || 0}) no coincide con nÃºmero de textos (${texts.length})`,
    );
  }

  return data.embeddings.map((emb: any) => emb.values);
}
```

**Alternativa con SDK (si estÃ¡ disponible):**

```typescript
// Si @google/genai SDK soporta embeddings en el futuro
import { GoogleGenAI } from "@google/genai";

const gemini = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
// Nota: Actualmente el SDK no tiene mÃ©todo directo para embeddings
```

---

### 3. Modificar `document-processor.ts`

**Cambios mÃ­nimos necesarios:**

```typescript
// src/features/documents/utils/document-processor.ts

// ... cÃ³digo de extracciÃ³n y chunking (MANTENER) ...

// 3. Generar embeddings con Gemini (CAMBIAR)
import { generateEmbeddingsWithGemini } from "./gemini-embeddings";

// Reemplazar esta secciÃ³n:
const embeddings = await generateEmbeddingsWithGemini(
  chunkedContent.map((chunk: { pageContent: string }) => chunk.pageContent),
);

// El resto del cÃ³digo se mantiene igual
```

---

### 4. DecisiÃ³n sobre Dimensiones de Embeddings

#### OpciÃ³n A: Cambiar a 768 dimensiones (Gemini)

**Ventajas:**

- âœ… Usa Gemini completamente
- âœ… Embeddings mÃ¡s pequeÃ±os (menos almacenamiento)
- âœ… MÃ¡s rÃ¡pido (menos dimensiones)

**Desventajas:**

- âŒ Requiere migraciÃ³n de BD
- âŒ Incompatible con embeddings existentes
- âŒ n8n podrÃ­a necesitar ajustes

**MigraciÃ³n SQL necesaria:**

```sql
-- 1. Crear nueva tabla temporal
CREATE TABLE documents_new (
  id bigserial PRIMARY KEY,
  content text,
  metadata jsonb,
  embedding vector(768)  -- Nueva dimensiÃ³n
);

-- 2. Migrar datos (requiere re-vectorizar)
-- Los embeddings existentes no se pueden convertir directamente

-- 3. Reemplazar tabla
DROP TABLE documents;
ALTER TABLE documents_new RENAME TO documents;

-- 4. Recrear Ã­ndices
CREATE INDEX documents_embedding_idx
ON documents USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

#### OpciÃ³n B: Mantener 1536 dimensiones (OpenAI)

**Ventajas:**

- âœ… Sin cambios en BD
- âœ… Compatible con datos existentes
- âœ… n8n sigue funcionando igual

**Desventajas:**

- âŒ No usa Gemini para embeddings
- âŒ Sigue dependiendo de OpenAI

**ImplementaciÃ³n:**

```typescript
// Usar Gemini solo para extracciÃ³n de texto (si es mejor)
// Mantener OpenAI para embeddings
const { embedMany } = await import("ai");
const { openai } = await import("@ai-sdk/openai");
// ... cÃ³digo actual ...
```

#### OpciÃ³n C: HÃ­brida - Gemini para ExtracciÃ³n, OpenAI para Embeddings

**Ventajas:**

- âœ… Usa Gemini donde es mejor (extracciÃ³n)
- âœ… Mantiene compatibilidad (1536 dimensiones)
- âœ… Sin cambios en BD

**ImplementaciÃ³n:**

```typescript
// Usar Gemini File Search Store temporalmente para extracciÃ³n
// Luego usar OpenAI para embeddings
// Mantener chunking con LangChain
```

---

## ğŸ—ï¸ Arquitectura Propuesta (OpciÃ³n Recomendada)

### OpciÃ³n Recomendada: **OpciÃ³n B o C**

**Razones:**

1. âœ… No requiere migraciÃ³n de BD
2. âœ… Compatible con n8n existente
3. âœ… No rompe funcionalidad actual
4. âœ… Puede migrar gradualmente

### Flujo Detallado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Procesamiento de Documentos                    â”‚
â”‚                  (Cuando se sube archivo)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Upload a Supabase Storage
   âœ… MANTENER (sin cambios)

2. /api/documents/vectorize
   âœ… MANTENER (sin cambios)

3. document-processor.ts
   â”œâ”€ ExtracciÃ³n de texto:
   â”‚  â”œâ”€ OpciÃ³n B: Mantener pdf-parse
   â”‚  â””â”€ OpciÃ³n C: Usar Gemini File Search Store temporal
   â”‚
   â”œâ”€ Chunking:
   â”‚  â””â”€ LangChain (MANTENER)
   â”‚
   â””â”€ Embeddings:
      â”œâ”€ OpciÃ³n B: OpenAI (MANTENER)
      â””â”€ OpciÃ³n C: Gemini REST API (si cambiamos a 768)

4. vector-store.ts
   â””â”€ Insertar en PostgreSQL
      âœ… MANTENER (sin cambios)

5. Esquema de BD
   â”œâ”€ OpciÃ³n B: vector(1536) - MANTENER
   â””â”€ OpciÃ³n C: vector(768) - REQUIERE MIGRACIÃ“N
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BÃºsqueda (NO SE TOCA)                      â”‚
â”‚              (Mantener flujo actual con n8n)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Usuario pregunta
   â†“
/api/chat
   â†“
Webhook a n8n
   â†“
n8n procesa:
   â”œâ”€ Genera embedding de query
   â”œâ”€ Busca en PostgreSQL (pgvector)
   â”œâ”€ Recupera chunks relevantes
   â””â”€ Genera respuesta
   â†“
Retorna al usuario
```

---

## ğŸ”§ ImplementaciÃ³n PrÃ¡ctica

### OpciÃ³n B: Mantener OpenAI (MÃ¡s Segura)

**Cambios mÃ­nimos:**

- âœ… No requiere cambios en BD
- âœ… No requiere cambios en n8n
- âœ… Compatible con todo existente

**ImplementaciÃ³n:**

```typescript
// document-processor.ts - NO CAMBIAR NADA
// Mantener cÃ³digo actual con OpenAI
```

**Ventaja:** Cero riesgo, cero cambios

---

### OpciÃ³n C: Gemini para ExtracciÃ³n Mejorada

**Cambios:**

1. Usar Gemini File Search Store temporalmente para extracciÃ³n
2. Mantener chunking con LangChain
3. Mantener embeddings con OpenAI (1536 dimensiones)

**ImplementaciÃ³n:**

```typescript
// document-processor.ts - MODIFICAR SOLO EXTRACCIÃ“N

export async function processDocumentForVectorization(
  fileUrl: string,
  fileType: string,
): Promise<{ chunks: Array<{ content: string; embedding: number[] }> }> {
  // 1. ExtracciÃ³n de texto
  let text: string;

  if (fileType === "application/pdf" || fileType.includes("pdf")) {
    // OPCIÃ“N: Usar Gemini File Search Store para extracciÃ³n
    // O mantener pdf-parse (mÃ¡s simple)

    // OpciÃ³n A: Mantener pdf-parse (actual)
    const response = await fetch(fileUrl);
    const arrayBuffer = await response.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);
    const pdfParse = require("pdf-parse");
    const data = await pdfParse(buffer);
    text = data.text;

    // OpciÃ³n B: Usar Gemini (mÃ¡s complejo, requiere store temporal)
    // const gemini = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });
    // ... crear store temporal, subir, extraer texto ...
  }

  // 2. Chunking (MANTENER)
  const { RecursiveCharacterTextSplitter } = await import(
    "@langchain/textsplitters"
  );
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  });
  const chunkedContent = await textSplitter.createDocuments([text]);

  // 3. Embeddings (MANTENER OpenAI)
  const { embedMany } = await import("ai");
  const { openai } = await import("@ai-sdk/openai");
  const { embeddings } = await embedMany({
    model: openai.embedding("text-embedding-3-small"),
    values: chunkedContent.map((chunk) => chunk.pageContent),
  });

  // 4. Retornar (MANTENER)
  return {
    chunks: chunkedContent.map((chunk, i) => ({
      content: chunk.pageContent,
      embedding: embeddings[i],
    })),
  };
}
```

---

## ğŸ“Š ComparaciÃ³n de Opciones

| Aspecto            | OpciÃ³n A (768)        | OpciÃ³n B (Mantener) | OpciÃ³n C (HÃ­brida) |
| ------------------ | --------------------- | ------------------- | ------------------ |
| **Cambios en BD**  | âŒ Requiere migraciÃ³n | âœ… Ninguno          | âœ… Ninguno         |
| **Compatibilidad** | âŒ Rompe existente    | âœ… Total            | âœ… Total           |
| **Uso de Gemini**  | âœ… Completo           | âŒ Ninguno          | âš ï¸ Parcial         |
| **Riesgo**         | ğŸ”´ Alto               | ğŸŸ¢ Bajo             | ğŸŸ¡ Medio           |
| **Complejidad**    | ğŸ”´ Alta               | ğŸŸ¢ Baja             | ğŸŸ¡ Media           |
| **RecomendaciÃ³n**  | âŒ No                 | âœ… **SÃ­**           | âš ï¸ Opcional        |

---

## ğŸ¯ RecomendaciÃ³n Final

### OpciÃ³n B: Mantener Todo Como EstÃ¡

**Razones:**

1. âœ… **Cero riesgo** - No rompe nada existente
2. âœ… **Cero cambios** - Funciona perfectamente como estÃ¡
3. âœ… **n8n intacto** - No requiere modificaciones
4. âœ… **BD intacta** - No requiere migraciÃ³n

**ConclusiÃ³n:**
Si el objetivo es usar Gemini pero **mantener el flujo de bÃºsqueda intacto**, entonces:

- **NO necesitas cambiar nada** en el procesamiento actual
- El flujo actual ya funciona perfectamente
- Gemini se puede usar en n8n si lo deseas (pero eso es configuraciÃ³n de n8n)

---

### Si Realmente Quieres Usar Gemini

**OpciÃ³n C (HÃ­brida) - Solo si hay beneficio real:**

1. **Usar Gemini para extracciÃ³n de texto** (si es mejor que pdf-parse)
   - Requiere File Search Store temporal
   - MÃ¡s complejo
   - Beneficio: Mejor extracciÃ³n de texto

2. **Mantener OpenAI para embeddings**
   - Compatible con esquema actual
   - n8n sigue funcionando
   - Sin cambios en BD

**ImplementaciÃ³n:**

- Crear funciÃ³n `extractTextWithGemini()` opcional
- Mantener `pdf-parse` como fallback
- Usar Gemini solo si hay beneficio medible

---

## ğŸ“ Plan de ImplementaciÃ³n (Si Procedes)

### Fase 1: EvaluaciÃ³n

1. âœ… Comparar calidad de extracciÃ³n: pdf-parse vs Gemini
2. âœ… Medir costos: OpenAI vs Gemini
3. âœ… Decidir si hay beneficio real

### Fase 2: ImplementaciÃ³n (Solo si hay beneficio)

1. âš ï¸ Crear funciÃ³n `extractTextWithGemini()` opcional
2. âš ï¸ Modificar `document-processor.ts` para usar Gemini opcionalmente
3. âš ï¸ Mantener pdf-parse como fallback
4. âš ï¸ Mantener OpenAI embeddings

### Fase 3: Testing

1. âš ï¸ Probar con documentos reales
2. âš ï¸ Comparar resultados
3. âš ï¸ Decidir si mantener cambio

---

## âš ï¸ Consideraciones Importantes

### 1. Gemini File Search Store es Temporal

**Problema:**

- File Search Stores se eliminan despuÃ©s de inactividad
- No es almacenamiento permanente
- Requiere recrear stores constantemente

**Impacto:**

- No es prÃ¡ctico para extracciÃ³n de texto
- Mejor mantener pdf-parse

### 2. Embeddings de Gemini Requieren REST API

**Problema:**

- `@google/genai` SDK no tiene embeddings directos
- Requiere usar REST API manualmente
- MÃ¡s complejo que OpenAI SDK

**Impacto:**

- OpenAI SDK es mÃ¡s simple
- Mantener OpenAI es mÃ¡s prÃ¡ctico

### 3. Dimensiones Diferentes

**Problema:**

- Gemini: 768 dimensiones
- OpenAI: 1536 dimensiones
- No son compatibles directamente

**Impacto:**

- Cambiar requiere migraciÃ³n completa
- Alto riesgo de romper funcionalidad

---

## âœ… ConclusiÃ³n

### RecomendaciÃ³n: **NO CAMBIAR NADA**

**Razones:**

1. El flujo actual funciona perfectamente
2. n8n maneja la bÃºsqueda correctamente
3. No hay beneficio claro de usar Gemini para procesamiento
4. Cambios introducen riesgo sin beneficio claro

### Si Insistes en Usar Gemini

**OpciÃ³n mÃ¡s segura:**

- Usar Gemini **dentro de n8n** para generaciÃ³n de respuestas
- Mantener procesamiento actual (OpenAI embeddings)
- Mantener bÃºsqueda actual (pgvector)

**Esto se hace en n8n, no en el cÃ³digo del proyecto.**

---

**Ãšltima actualizaciÃ³n**: Enero 2025
**VersiÃ³n del anÃ¡lisis**: 2.0 (Corregido)
